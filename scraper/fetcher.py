from playwright.sync_api import sync_playwright
import time
import os

AUTH_FILE = "auth_threads.json"


def deep_scroll_comments(page, max_loops: int = 15):
    """
    æ·±åº¦æ²å‹•é é¢ä¸¦å˜—è©¦å±•é–‹æ›´å¤šç•™è¨€ / å›è¦†ã€‚
    - é€éæ»‘é¼ æ»¾å‹•å‘ä¸‹è¼‰å…¥æ›´å¤šå…§å®¹
    - å˜—è©¦é»æ“Š "View more replies" / "View more" / "Show replies"
    - è‹¥ scrollHeight å¤šæ¬¡æœªè®ŠåŒ–å‰‡æå‰åœæ­¢
    """
    stable_count = 0
    last_height = 0
    expand_texts = ["View more replies", "View more", "Show replies"]

    for _ in range(max_loops):
        page.mouse.wheel(0, 3000)
        page.wait_for_timeout(1500)

        for text in expand_texts:
            try:
                for btn in page.get_by_text(text, exact=False).all():
                    btn.click(timeout=2000)
                    page.wait_for_timeout(500)
            except Exception:
                # å¿½ç•¥æ‰¾ä¸åˆ°æˆ–é»æ“Šå¤±æ•—ï¼Œç¹¼çºŒä¸‹ä¸€å€‹
                pass

        height = page.evaluate("document.body.scrollHeight")
        if height == last_height:
            stable_count += 1
        else:
            stable_count = 0
        last_height = height

        if stable_count >= 3:
            break


def normalize_url(url: str) -> str:
    # å¦‚æœæ˜¯ threads.comï¼Œå°±è‡ªå‹•æ”¹æˆ threads.net
    if "threads.com" in url:
        new_url = url.replace("threads.com", "threads.net")
        print(f"ğŸ” åµæ¸¬åˆ° threads.comï¼Œå·²è‡ªå‹•æ”¹æˆï¼š{new_url}")
        return new_url
    return url

def fetch_page_html(url: str) -> str:
    """
    Step 1: æ‰“é–‹ Threads ç¶²é 
    Step 2: ç”¨ storage_state (auth_threads.json) ç™»å…¥
    Step 3: å›å‚³å®Œæ•´ HTML å­—ä¸²
    """
    
    if not os.path.exists(AUTH_FILE):
        raise FileNotFoundError("âš ï¸ æ‰¾ä¸åˆ° auth_threads.jsonï¼Œè«‹å…ˆåŸ·è¡Œ login.pyã€‚")

    url = normalize_url(url)
    html_content = ""

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(storage_state=AUTH_FILE)
        page = context.new_page()

        try:
            print(f"ğŸ•¸ï¸ æ­£åœ¨è¼‰å…¥ {url} ...")
            response = page.goto(url, timeout=60000, wait_until="load")
            
            if response is None:
                print("âš ï¸ æ²’æœ‰æ‹¿åˆ°ä»»ä½• HTTP å›æ‡‰ (response is None)")
                browser.close()
                return ""

            status = response.status
            print(f"ğŸ“¡ HTTP ç‹€æ…‹ç¢¼ï¼š{status}")

            if status < 200 or status >= 300:
                print(f"âŒ é 2xx å›æ‡‰ï¼ˆå¯èƒ½æ˜¯ 404/403/500 ç­‰ï¼‰ï¼Œç„¡æ³•æŠ“å–æ­¤é ã€‚")
                browser.close()
                return ""

            page.wait_for_load_state("networkidle")
            time.sleep(3)  # è®“ç•«é¢å¤šè¼‰å…¥ä¸€äº›
            print("ğŸ” æ·±åº¦æ²å‹•ç•™è¨€å€...")
            deep_scroll_comments(page)

            html_content = page.content()
            print(f"âœ… æŠ“åˆ° HTMLï¼Œé•·åº¦ï¼š{len(html_content)} å­—å…ƒ")

        except Exception as e:
            print(f"âŒ Fetch Error: {e}")
        finally:
            browser.close()

    return html_content
